\documentclass[pdf]{beamer}
\mode<presentation>{}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{lmodern}


%sources:

%http://www.vlab.msi.umn.edu/people/download/InternshipSummer2007/handler%20pres.pdf
%http://qubit-ulm.com/wp-content/uploads/2012/04/Lanczos_Algebra.pdf
%http://www.ams.org/bookstore/pspdf/mbk-76-prev.pdf

\usetheme{Warsaw}
\usecolortheme{dolphin}

\title{The Lanczos Algorithm}
\author{Shane Harding}

\begin{document}


\begin{frame}
\titlepage
\end{frame}

%%%%%%

\begin{frame}
\frametitle{Outline of talk}
\tableofcontents[]

\end{frame}

%%%%%%%%%%%%%%%%%	INTRO	%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\begin{frame}
\tableofcontents[currentsection]
\end{frame}



\begin{frame}
\frametitle{Introduction}
\centering
The Lanczos Algorithm is a very efficient method of finding, a few, `extreme' eigenvalues of symmetric matrices, $A$.

By extreme, we mean the eigenvalues with the largest and smallest ablsoute values. The algorithm will only find a small number, $m$, of the total number, $n$, of eigenvalues.

\end{frame}



\begin{frame}
\frametitle{What is an Eigenvalue}

If we consider the matrix equation: $A x = b$. Then $x$ is an eigenvector if, and only if, applying $A$ to $x$ yields a scalar multiple of $x$. The factor $x$ is rescaled by is called the eigenvalue and is denoted $\lambda$.

\[ A x = \lambda x \]

\end{frame}



\begin{frame}
\centering
\frametitle{Brief History}

The Lanczos algorithm was developed by Cornelius Lanczos. He developed the method while working at the National Bureau of Standards in Washington DC. It was then forgotten for a number of years due to the lack of computers meaning that it couldn't be properly utilised. When it was then ``rediscovered" it was modified multiple times to fix some instability issues.
\end{frame}



\begin{frame}
\centering
\frametitle{Brief History}

Lanczos later joined the Institute of Numerical Analysis (INA). While working there he entered correspondence with Erwin Schr\"{o}dinger, who was the director of the Dublin Institute of Advanced Studies (DIAS) at the time. Schro\"{o}dinger gave him one year visiting professorship at DIAS. In 1954 Eamon de Valera invited Lanczos to be a senior professor in the School of Theoretical Physics.

\end{frame}


%%%%%%%%%%%%%	THE ALGORITHM	%%%%%%%%%%%%%%%%%%%%%


\section{The Algorithm}
\begin{frame}
\tableofcontents[currentsection]
\end{frame}



\begin{frame}
The Lanczos Algorithm is used to solve large scale eigenvalue problems. So give a large $n \times n$ matrix $A$ we have:
\begin{exampleblock}{Eigenvalues and eigenvectors}
\[
A v_i = \lambda_i v_i
\]
\end{exampleblock}
Where the vectors $v_i$ are the eigenvectors and the scalars $\lambda_i$ are the eigenvalues.
\end{frame}


\begin{frame}
\frametitle{Power Iteration}
\begin{itemize}

\item The Lanczos algorithm is an adaptation of the power method.
\item The power methods (or power iteration or Von Mises iteration) is an eigenvalue algorithm.
\item This algorithm will only produce one eigenvalue and may converge slowly.
\item It finds the eigenvalue with the greatest absolute value.

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Power Iteration}

The power method can be summarised as follows: If $x_0$ is some random vector and $x_{n+1}=A x_n$ then if we consider the limit of $n$ being large we find that $\frac{x_n}{||x_n||}$ approaches the normed eigenvector with the greatest eigen value. 
\begin{itemize}
\item Then if $A=U {diag} (\sigma_i) U'$ is the eigendeecomposition of $A$ then $A^n = U {diag}(\sigma^n_i) U'$.
\item As n gets big this will be dominated by the largest eigenvalue
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{The Lanczos Algorithm}
\begin{itemize}
\item We wish to find the eigenvalues of the matrix $A$.

\item Where $A \in \mathbb{R}^{N \times N}$ and $v_1 \in \mathbb{R}^N$ be a random vector, with norm one.

\item We will calculate the tridiagonal, symmetric matrix $T_{mm} = V_m^* A V_m$. With the diagonal elements denoted $\alpha_i=t_{ii}$ and the off diagonal elements given by $\beta_i = t_{i-1,i}$. And note that $t_{i-1,i}=t_{i,i-1}$ due to symmetry.
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{The Lanczos Algorithm}
\begin{verbatim}
1. v_1 = random vec; v_0 = 0; beta_1 = 0;
2. for i = 1,...,m -1do
     w_i = A v_i
     alpha_i = (r_i,v_i)
     w_i = w_i - alpha_i v_i - beta_i v_{i=1}
     beta_{i+1} = abs(w_i)
     v_{i+1} = w_i/beta_{i+1}
   endfor
3. w_m = A v_m
   alpha_m = (w_m,v_m)
   return
\end{verbatim}

Where $(x,y)$ represents the dot product of $x$ and $y$. $m$ is chosen by the user. It is the number of eigenvalues that want to be obtained.


\end{frame}



\begin{frame}
\frametitle{The Lanczos Algorithm}

The previous routine yields the matrix:

\[ T_{mm} = \left( \begin{array}{cccc}
\alpha_1 & \beta_{2} & & 0\\
\beta_{2} & \ddots &\ddots & \\
& \ddots & \ddots & \beta_{m} \\
0 & & \beta_{m,} & \alpha_m \end{array} \right) \]

The vectors $v_i$ are called the Lanczos vectors. They are then used to construct the transformation matrix $V_m=(v_1,v_2,\dots,v_m)$. This is very useful for computing the eigenvectors.

\end{frame}


\begin{frame}
\frametitle{The Lanczos Algorithm}
\begin{itemize}
\item The vectors $v_i$ in our algorithm should all be orthogonal to each other. Rounding errors however will grow over time so every few iterations a reorthogonalisation process should be run.
\item Once $T_{mm}$ is obtained, its eigenvalues $\lambda^{(m)}_i$ and their matching eigenvectors $u^{(m)}_i$ can be calculated.
\item There are a few commonly used methods to do this. Two examples are: multiple relatively robust representations (MRRR); and QR algorithm.

\item It can be proved that the eigenvalues of $T_{mm}$ are approximate eigenvalues of the original matrix $A$.
\end{itemize}

\end{frame}




%%%%%%%%%%%%%%%%%%%	PALLEL LANCZOS ALGORITHM	%%%%%%%%%%%%%%%

\section{Parallel Lanczos Algorithm}

\begin{frame}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}


\end{frame}


%%%%%%%%%%%%%%%%%%%	APPLICATIONS	%%%%%%%%%%%%%%%%%%%%


\section{Applications}

\begin{frame}
\tableofcontents[currentsection]
\end{frame}


\begin{frame}
\frametitle{Applications}
There are many many applications of the Lanczos method. It is a popular method in Condensed Matter Physics where the Hamiltonians of electron systems need to be solved. It is also in solving non-linear inverse problems, which is very useful in modeling oil and gas reservoirs.

Eigenvectors are important in large scale ranking methods. Google's PageRank algorithm is good example of this. 
\end{frame}


\begin{frame}
\frametitle{Google PageRank Algorithm}

When you make a Google search, Google uses an algorithm, called PageRank, to rank the results of your search. This is what Google says the PageRank algorithm is:

\begin{exampleblock}{PageRank}
PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites 
\end{exampleblock}

\end{frame}



\begin{frame}
\frametitle{Google PageRank Algorithm}
\begin{itemize}
\item PageRank gives a probability distribution that represents the likelihood that if you randomly click on links you will arrive on a given page. 
\item Each page, $u$ is given a value, $PR(u)$, between zero and one; and the sum over all the $PR$'s must be one.
\item The initial value of $PR(u)$ is simply one divided by the number of pages for each page $u$. So if there are 12 pages, each $PR(u_i)$ is $\frac{1}{12}$.
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Google PageRank Algorithm}
\begin{itemize}
\item Let $B_p$ be the set of all the pages that link to the page $p$.
\item Let $L(q)$ be the number of unique outgoing links from a page $q$.
\item Then the general PageRank value can be written as: \[PR(p) = \sum_{q \in B_p} \frac{PR(q)}{L(q)} \]
\item This is then modified to include a `damping' factor: \[ PR(p_1) = \frac{1-d}{N} + d \left( \frac{PR(p_2)}{L(p_2)} + \frac{PR(p_3)}{L(p_3)} + \dots \right) \]
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Where's Lanczos?}

Can write a column vector $R$ as: \[R = \left( \begin{array}{c} PR(p_1) \\ \vdots \\ PR(p_N) \end{array} \right) \]

Then we can write the equation from the last slide as:

\[ R = \left( \begin{array}{c} \frac{1-d}{N} \\ \vdots \\ \frac{1-d}{N} \end{array} \right) +d \left( \begin{array}{cccc}
l(p_1,p_1) & l(p_1,p_2) & \dots & l(p_1,p_N)\\
l(p_2,p_1) & \ddots & & \vdots \\
\vdots & & l(p_i,p_j) & \\
l(p_N,p_1) & \dots & & l(p_N,p_N)\\
 \end{array} \right) R \]
\end{frame}

\begin{frame}

\end{frame}



%%%%%%%%%%%%%%%		QUESTIONS	%%%%%%%%%

\section{Questions}

\begin{frame}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}
\centering
Questions?
\end{frame}

\begin{frame}
\centering
Thank you!
\end{frame}

\end{document}
