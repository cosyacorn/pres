\documentclass[pdf]{beamer}
\mode<presentation>{}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{lmodern}


%sources:

%http://www.vlab.msi.umn.edu/people/download/InternshipSummer2007/handler%20pres.pdf
%http://qubit-ulm.com/wp-content/uploads/2012/04/Lanczos_Algebra.pdf
%http://www.ams.org/bookstore/pspdf/mbk-76-prev.pdf

\usetheme{Warsaw}
\usecolortheme{dolphin}

\title{The Lanczos Algorithm}
\author{Shane Harding}

\begin{document}


\begin{frame}
\titlepage
\end{frame}

%%%%%%

\begin{frame}
\frametitle{Outline of talk}
\tableofcontents[]

\end{frame}

%%%%%%%%%%%%%%%%%	INTRO	%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\begin{frame}
\tableofcontents[currentsection]
\end{frame}



\begin{frame}
\frametitle{Introduction}
\centering
The Lanczos Algorithm is a very efficient method of finding, a few, `extreme' eigenvalues of symmetric matrices, $A$.

By extreme, we mean the eigenvalues with the largest and smallest ablsoute values. The algorithm will only find a small number, $m$, of the total number, $n$, of eigenvalues.

\end{frame}



\begin{frame}
\frametitle{What is an Eigenvalue}

If we consider the matrix equation: $A x = b$. Then $x$ is an eigenvector if, and only if, applying $A$ to $x$ yields a scalar multiple of $x$. The factor $x$ is rescaled by is called the eigenvalue and is denoted $\lambda$.

\[ A x = \lambda x \]

\end{frame}



\begin{frame}
\centering
\frametitle{Brief History}

The Lanczos algorithm was developed by Cornelius Lanczos. He developed the method while working at the National Bureau of Standards in Washington DC. It was then forgotten for a number of years due to the lack of computers meaning that it couldn't be properly utilised. When it was then ``rediscovered" it was modified multiple times to fix some instability issues.
\end{frame}



\begin{frame}
\centering
\frametitle{Brief History}

Lanczos later joined the Institute of Numerical Analysis (INA). While working there he entered correspondence with Erwin Schr\"{o}dinger, who was the director of the Dublin Institute of Advanced Studies (DIAS) at the time. Schro\"{o}dinger gave him one year visiting professorship at DIAS. In 1954 Eamon de Valera invited Lanczos to be a senior professor in the School of Theoretical Physics.

\end{frame}


%%%%%%%%%%%%%	THE ALGORITHM	%%%%%%%%%%%%%%%%%%%%%


\section{The Algorithm}
\begin{frame}
\tableofcontents[currentsection]
\end{frame}



\begin{frame}
The Lanczos Algorithm is used to solve large scale eigenvalue problems. So give a large $n \times n$ matrix $A$ we have:
\begin{exampleblock}{Eigenvalues and eigenvectors}
\[
A v_i = \lambda_i v_i
\]
\end{exampleblock}
Where the vectors $v_i$ are the eigenvectors and the scalars $\lambda_i$ are the eigenvalues.
\end{frame}


\begin{frame}
\frametitle{Power Iteration}
\begin{itemize}

\item The Lanczos algorithm is an adaptation of the power method.
\item The power methods (or power iteration or Von Mises iteration) is an eigenvalue algorithm.
\item This algorithm will only produce one eigenvalue and may converge slowly.
\item It finds the eigenvalue with the greatest absolute value.

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Power Iteration}

The power method can be summarised as follows: If $x_0$ is some random vector and $x_{n+1}=A x_n$ then if we consider the limit of $n$ being large we find that $\frac{x_n}{||x_n||}$ approaches the normed eigenvector with the greatest eigen value. 
\begin{itemize}
\item Then if $A=U {diag} (\sigma_i) U'$ is the eigendeecomposition of $A$ then $A^n = U {diag}(\sigma^n_i) U'$.
\item As n gets big this will be dominated by the largest eigenvalue
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{The Lanczos Algorithm}
\begin{itemize}
\item We wish to find the eigenvalues of the matrix $A$.

\item Where $A \in \mathbb{R}^{N \times N}$ and $v_1 \in \mathbb{R}^N$ be a random vector, with norm one.

\item We will calculate the tridiagonal, symmetric matrix $T_{mm} = V_m^* A V_m$. With the diagonal elements denoted $\alpha_i=t_{ii}$ and the off diagonal elements given by $\beta_i = t+{i-1,i}$. And note that $t_{i-1,i}=t_{i,i-1}$ due to symmetry.
\end{itemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{The Lanczos Algorithm}
\begin{verbatim}
1. v_1 = random vec; v_0 = 0; beta_1 = 0;
2. for i = 1,...,m -1do
     w_i = A v_i
     alpha_i = (r_i,v_i)
     w_i = w_i - alpha_i v_i - beta_i v_{i=1}
     beta_{i+1} = abs(w_i)
     v_{i+1} = w_i/beta_{i+1}
   endfor
3. w_m = A v_m
   alpha_m = (w_m,v_m)
   return
\end{verbatim}

Where $(x,y)$ represents the dot product of $x$ and $y$.

$m$ is chosen by the user. It is the number of eigenvalues that want to be obtained.


\end{frame}



\begin{frame}
\frametitle{The Lanczos Algorithm}

The previous routine yields the matrix:

\[ T_{mm} = \left( \begin{array}{cccc}
\alpha_1 & \beta_{2} & & 0\\
\beta_{2} & \ddots &\ddots & \\
& \ddots & \ddots & \beta_{m} \\
0 & & \beta_{m,} & \alpha_m \end{array} \right) \]

The vectors $v_i$ are called the Lanczos vectors. They are then used to construct the transformation matrix $V_m=(v_1,v_2,\dots,v_m)$. This is very useful for computing the eigenvectors.

\end{frame}


\begin{frame}
\frametitle{The Lanczos Algorithm}

Once $T_{mm}$ is obtained, its eigenvalues $\lambda^(m)_i$ and their matching eigenvectors $u^(m)_i$ can be calculated. There are a few commonly used methods to do this. Two examples are: multiple relatively robust representations (MRRR); and QR algorithm.

It can be proved that the eigenvalues of $T_{mm}$ are approximate eigenvalues of the original matrix $A$.


\end{frame}




%%%%%%%%%%%%%%%%%%%	PALLEL LANCZOS ALGORITHM	%%%%%%%%%%%%%%%

\section{Parallel Lanczos Algorithm}

\begin{frame}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}


\end{frame}


%%%%%%%%%%%%%%%%%%%	APPLICATIONS	%%%%%%%%%%%%%%%%%%%%


\section{Applications}

\begin{frame}
\tableofcontents[currentsection]
\end{frame}

%%%%%%%%%%%%%%%		QUESTIONS	%%%%%%%%%

\section{Questions}

\begin{frame}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}
\centering
Questions?
\end{frame}

\begin{frame}
\centering
Thank you!
\end{frame}

\end{document}
